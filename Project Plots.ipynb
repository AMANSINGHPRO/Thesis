{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk                                                                                #import nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown                                                              #import corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Editorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "editorial_tagged_sents = brown.tagged_sents(categories='editorial')                         #tagged sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "editorial_sents = brown.sents(categories='editorial')                                       #untagged sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "editorial_fd = nltk.FreqDist(brown.words(categories='editorial'))                           #frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "editorial_cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='editorial'))              #cumulative frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "most_freq_words = editorial_fd.most_common(100)                                             #top 100 frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "likely_tags = dict((word, editorial_cfd[word].max()) for (word, _) in most_freq_words)      #Likelihood of tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags)                                     #Unigram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4754236737874164"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger.evaluate(editorial_tagged_sents)                                            #evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = brown.sents(categories='editorial')[6]                                                  #set sentence #6 from editorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('session', None),\n",
       " (',', ','),\n",
       " ('for', 'IN'),\n",
       " ('instance', None),\n",
       " (',', ','),\n",
       " ('may', 'MD'),\n",
       " ('have', 'HV'),\n",
       " ('insured', None),\n",
       " ('a', 'AT'),\n",
       " ('financial', None),\n",
       " ('crisis', None),\n",
       " ('two', None),\n",
       " ('years', 'NNS'),\n",
       " ('from', 'IN'),\n",
       " ('now', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger.tag(sent)                                                                   # tag the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags,\n",
    "...                                      backoff=nltk.DefaultTagger('NN'))                          #set backoff to default tagger 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(cfd, wordlist):                                                              #define performance\n",
    "    lt = dict((word, cfd[word].max()) for word in wordlist)\n",
    "    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.DefaultTagger('NN'))\n",
    "    return baseline_tagger.evaluate(brown.tagged_sents(categories='editorial'))              \n",
    "\n",
    "def display():                                                                               #define display \n",
    "    import pylab                                                                             #import library\n",
    "    word_freqs = nltk.FreqDist(brown.words(categories='editorial')).most_common()            \n",
    "    words_by_freq = [w for (w, _) in word_freqs]\n",
    "    cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='editorial'))               \n",
    "    sizes = 2 ** pylab.arange(15)\n",
    "    perfs = [performance(cfd, words_by_freq[:size]) for size in sizes]\n",
    "    pylab.plot(sizes, perfs, '-bo')\n",
    "    pylab.title('Lookup Tagger Performance with Varying Model Size for editorial')            #set title\n",
    "    pylab.xlabel('Model Size')                                                                #set label\n",
    "    pylab.ylabel('Performance')                                                               #set label\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display()                                                                                     #display output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_tagged_sents = brown.tagged_sents(categories='news')                         #tagged sents\n",
    "news_sents = brown.sents(categories='news')                                       #untagged sents\n",
    "news_fd = nltk.FreqDist(brown.words(categories='news'))                           #frequency distribution\n",
    "news_cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))        #cumulative frequency distribution\n",
    "most_freq_words = news_fd.most_common(100)                                        #top 100 frequent words\n",
    "likely_tags = dict((word, news_cfd[word].max()) for (word, _) in most_freq_words) #Likelihood of tags\n",
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags)                           #Unigram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45578495136941344"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger.evaluate(news_tagged_sents)                                       #evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent = brown.sents(categories='news')[4]                                          #set sentence #4 from news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('jury', None),\n",
       " ('said', 'VBD'),\n",
       " ('it', 'PPS'),\n",
       " ('did', None),\n",
       " ('find', None),\n",
       " ('that', 'CS'),\n",
       " ('many', None),\n",
       " ('of', 'IN'),\n",
       " (\"Georgia's\", None),\n",
       " ('registration', None),\n",
       " ('and', 'CC'),\n",
       " ('election', None),\n",
       " ('laws', None),\n",
       " ('``', '``'),\n",
       " ('are', 'BER'),\n",
       " ('outmoded', None),\n",
       " ('or', 'CC'),\n",
       " ('inadequate', None),\n",
       " ('and', 'CC'),\n",
       " ('often', None),\n",
       " ('ambiguous', None),\n",
       " (\"''\", \"''\"),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger.tag(sent)                                                         #tag the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags,\n",
    "...                                      backoff=nltk.DefaultTagger('NN'))           #set backoff to default tagger 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(cfd, wordlist):                                                      #define performance\n",
    "    lt = dict((word, cfd[word].max()) for word in wordlist)\n",
    "    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.DefaultTagger('NN'))\n",
    "    return baseline_tagger.evaluate(brown.tagged_sents(categories='news'))\n",
    "\n",
    "def display():                                                                       #define display \n",
    "    import pylab                                                                     #import library\n",
    "    word_freqs = nltk.FreqDist(brown.words(categories='news')).most_common()\n",
    "    words_by_freq = [w for (w, _) in word_freqs]\n",
    "    cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='news'))\n",
    "    sizes = 2 ** pylab.arange(15)\n",
    "    perfs = [performance(cfd, words_by_freq[:size]) for size in sizes]\n",
    "    pylab.plot(sizes, perfs, '-bo')\n",
    "    pylab.title('Lookup Tagger Performance with Varying Model Size for news')         #set title\n",
    "    pylab.xlabel('Model Size')                                                        #set label\n",
    "    pylab.ylabel('Performance')                                                       #set label\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display() #display output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews_tagged_sents = brown.tagged_sents(categories='reviews')                       #tagged sents\n",
    "reviews_sents = brown.sents(categories='reviews')                                     #untagged sents\n",
    "reviews_fd = nltk.FreqDist(brown.words(categories='reviews'))                         #frequency distribution\n",
    "reviews_cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='reviews'))      #cumulative frequency distribution\n",
    "most_freq_words = reviews_fd.most_common(100)                                         #top 100 frequent words\n",
    "likely_tags = dict((word, reviews_cfd[word].max()) for (word, _) in most_freq_words)  #Likelihood of tags\n",
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags)                               #Unigram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47494103773584906"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger.evaluate(reviews_tagged_sents)                                        #evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = brown.sents(categories='reviews')[4]                                           #set sentence #4 from reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Those', None),\n",
       " ('would', 'MD'),\n",
       " ('be', 'BE'),\n",
       " ('reserved', None),\n",
       " ('for', 'IN'),\n",
       " ('the', 'AT'),\n",
       " (\"orchestra's\", None),\n",
       " ('great', 'JJ'),\n",
       " ('nights', None),\n",
       " ('when', 'WRB'),\n",
       " ('the', 'AT'),\n",
       " ('soloist', None),\n",
       " ('can', 'MD'),\n",
       " ('surpass', None),\n",
       " ('himself', None),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger.tag(sent)                                                             #tag the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags,\n",
    "...                                      backoff=nltk.DefaultTagger('NN'))            #set backoff to default tagger 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(cfd, wordlist):                                                       #define performance\n",
    "    lt = dict((word, cfd[word].max()) for word in wordlist)\n",
    "    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.DefaultTagger('NN'))\n",
    "    return baseline_tagger.evaluate(brown.tagged_sents(categories='reviews'))\n",
    "\n",
    "def display():                                                                        #define display \n",
    "    import pylab                                                                      #import library\n",
    "    word_freqs = nltk.FreqDist(brown.words(categories='reviews')).most_common()\n",
    "    words_by_freq = [w for (w, _) in word_freqs]\n",
    "    cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='reviews'))\n",
    "    sizes = 2 ** pylab.arange(15)\n",
    "    perfs = [performance(cfd, words_by_freq[:size]) for size in sizes]\n",
    "    pylab.plot(sizes, perfs, '-bo')\n",
    "    pylab.title('Lookup Tagger Performance with Varying Model Size for reviews')       #set title\n",
    "    pylab.xlabel('Model Size')                                                         #set label\n",
    "    pylab.ylabel('Performance')                                                        #set label\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display()                                                                              #display output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Government"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "government_tagged_sents = brown.tagged_sents(categories='government')                   #tagged sents\n",
    "government_sents = brown.sents(categories='government')                                 #untagged sents\n",
    "government_fd = nltk.FreqDist(brown.words(categories='government'))                     #frequency distribution\n",
    "government_cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='government'))  #cumulative frequency distribution\n",
    "most_freq_words = government_fd.most_common(100)                                        #top 100 frequent words\n",
    "likely_tags = dict((word, government_cfd[word].max()) for (word, _) in most_freq_words) #Likelihood of tags\n",
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags)                                 #Unigram Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4654791277436285"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger.evaluate(government_tagged_sents)                                      #evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = brown.sents(categories='government')[4]                                         #set sentence #4 from government"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('For', None),\n",
       " ('further', None),\n",
       " ('information', None),\n",
       " ('contact', None),\n",
       " ('Director', None),\n",
       " (',', ','),\n",
       " ('Office', None),\n",
       " ('of', 'IN'),\n",
       " ('Business', None),\n",
       " ('Economics', None),\n",
       " (',', ','),\n",
       " ('U.S.', None),\n",
       " ('Department', 'NN-TL'),\n",
       " ('of', 'IN'),\n",
       " ('Commerce', None),\n",
       " (',', ','),\n",
       " ('Washington', None),\n",
       " ('25', None),\n",
       " (',', ','),\n",
       " ('D.C.', None),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_tagger.tag(sent)                                                              #tag the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags,\n",
    "...                                      backoff=nltk.DefaultTagger('NN'))            #set backoff to default tagger 'NN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def performance(cfd, wordlist):                                                       #define performance\n",
    "    lt = dict((word, cfd[word].max()) for word in wordlist)\n",
    "    baseline_tagger = nltk.UnigramTagger(model=lt, backoff=nltk.DefaultTagger('NN'))\n",
    "    return baseline_tagger.evaluate(brown.tagged_sents(categories='government'))\n",
    "\n",
    "def display():                                                                        #define display \n",
    "    import pylab                                                                      #import library\n",
    "    word_freqs = nltk.FreqDist(brown.words(categories='government')).most_common()\n",
    "    words_by_freq = [w for (w, _) in word_freqs]\n",
    "    cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories='government'))\n",
    "    sizes = 2 ** pylab.arange(15)\n",
    "    perfs = [performance(cfd, words_by_freq[:size]) for size in sizes]\n",
    "    pylab.plot(sizes, perfs, '-bo')\n",
    "    pylab.title('Lookup Tagger Performance with Varying Model Size for government')   #set title\n",
    "    pylab.xlabel('Model Size')                                                        #set label\n",
    "    pylab.ylabel('Performance')                                                       #set label\n",
    "    pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display()                                                                             #display output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End\n",
    "## Reference:\n",
    "### Bird,  S.,  Klein,  E.  and  Loper,  E.  (2009).   Natural  language  processing  with  python,O’Reilly Media, Inc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
